依据任务抽象出不同的正负案例

## 理论分析
数据要求：
    1.数据量 2.质量高【对本次实验要有更好的明显特征】
数据标注：
    不同实验对物体的标注目的性不同，需要根据实验需求来标注。
    如：检测物体就需要对每个物体分别标注，检测分类就需要对相同的类标注相同

数据集train, valid, test，三个数据集不能有重叠部分：
train（60%-80%）：训练模型，让模型学习规律、调整内部参数。
valid（10%-20%）：调试超参数，选择最佳模型。评估不同模型配置的表现，防止过拟合，帮助决定何时停止训练。
test（10%-20%）：检验最终训练效果，是黑盒训练

使用流程：
1.使用train训练数据
2.用valid验证评估->调整超参数->重新训练
3.重复步骤1-2，直到valid表现满意
4.使用test评估shu

数据集查看注意点：
1.查看数据集中的file文件信息、id信息、标注的信息
2.图片标注的信息：包括难易区分程度、是否多目标、是否完整标注、标注的x、y轴坐标的含义
3.注意x、y轴起始点定义的区间
4.注意标注的物体类别，对于不同任务需求的，标注的信息和类别不同

YOLO格式的标注，对于目标位置的标注：<class_id, yolo_center_x, yolo_center_y, yolo_width, yolo_height>
1.YOLO关注目标的中心坐标(x, y)，再进行归一化操作（即百分比）
2.归一化操作：
    yolo_center_x = center_x/ image_width, yolo_center_y = center_y/ image_width，
    yolo_width = width/ image_width, yolo_height = height/ image_width
    注意此处的x和y均是中心坐标的位置, width和height分别是目标框的总体宽度和高度

网络模型结构 -- 积木【了解作用，输入、输出】
网络模型 -- 用积木搭建的

CNN模型参数【以图像2d数据为例】
**经典的CNN顺序:conv(卷积)->BN(batch normalization)->relu(激活)->conv**

1. 卷积层
in_channels: 输入的参数通道数，彩色图像一般为RGB，通道数为3也就是行数为3，对以下图来说
    R卷积R，G卷积G，B卷积B.
out_channels: 需要输出的结果通道数，即列数为out_channels
![img.png](noteSource/in_and_out.png "kernels")
kenel_size：一般是3，即3*3，也就是卷积块的大小，也可以是元组（5, 2）即height=5 * width=3
strides: 默认步长（1， 1）表示对输入图像（纵向步长， 横向步长），若步长（1， 3）表示行差距1，列差距3
padding: 对图像周边【以】填充的图像，一般填充的padding是0
dilation: 指明各个卷积核之间在输入图像之间的间距是多少，（4， 2）表示行差距是4，列差距是2
groups:

padding增大时，能增大宽与高
stride增大时，能减小宽与高
dilation增大时，能减小宽与高

！！！注意：卷积操作时一般用奇数--3和5主要是用来提取特征，1主要是改变通道数

2. 池化层(pooling)【以2d图片为例】
池化的作用是减少数据量并保留重要数据的方法，把原本的数据做一个最大池化或平均池化的降维操作，即减低图片分辨率。
简而言之就是减少数据维度，保留关键信息，在一定程度上控制过拟合。
池化层的引入是仿照人的视觉系统对视觉输入对象进行降维和抽象，池化层的三个功效：
a.特征不变性:池化操作是模型更加关注是否存在某些
最大/小池化则是取出范围内的最大/小值，平均池化则是取出范围内的平均数。

下采样：从高分辨率到低分辨率
上采样：从低分辨率到高分辨率

3. 填充层(padding)
例如:
ReflectionPad2d( (left, right, top, bottom) )传入的是元组数据
如果ReflectionPad2d(2),那么左右上下填充的都是2个元素

4. 非线性激活(activation)
非线性激活函数使网络能够学习和表示复杂的非线性关系，通过非线性变换，神经网络可以拟合各种复杂的函数，解决如图像识别、自然语言处理等复杂问题。

a. 常用激活函数及其范围
sigmoid()(0, 1)，适合概率输出
tanh()(-1, 1)，数据中心化
relu()[0, +∞)，保持正值
leakyRelu()

5. 归一化层(normalization BN)
即进行特征缩放，将不同的数值归一化到0-1之间，减少梯度爆炸或收敛慢的问题，固定输入分布，避免内部协变量偏移，加速网络的收敛速度。
例batchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
num_features 是指通道数
(N, C, H, W)四维解释
N:batch大小,一批送入训练的图片的数量,一般是2的整数倍
C:每个图片的通道数
H:高
W:宽

对送入的batch图片依次进行channel操作,即各个channel依次进行归一化操作
计算的均值和标准差是对batch内的同一个channel数据进行的计算

6. 循环层


7. Transformer层


8. 线性层(linear, 或全连接层fully connected layer)
在对于linear之前需要进行flatten展平操作，全连接层的每一个结点都与上一层的所有结点连接，也是参数最多的一层
线性层（或全连接层）一般位于整个卷积神经网络的最后，负责将卷积输出的二维特征图【输入图像或音频等信息】转化成一维向量【输出一个向量或信息】，实现端到端的过程。

全连接层的作用：将前层(卷积、池化等层)计算得到的特征空间映射样本标记空间
    a. 将特征表示整合成一个值，减少特征位置对分类结果的影响，提高整个网络的鲁棒性
    b. 全连接在整个网络中起到“分类器”的作用，将特征整合到一起，方便交给最后的分类器或回归
全连接层的实现，可由卷积层实现：
    a.对前层是全连接的全连接层可以转换为卷积核为1*1的卷积
    b.对前层是卷积层的全连接层可以转换为卷积核为前层卷积输出结果的宽和高一样大小的全局卷积

in_features * wi+ bi【参考https://blog.csdn.net/qq_44722189/article/details/135035351】

参数：
in_features 每个输入样本的大小【输入特征】
out_features 每个输出样本的大小【输出的神经元个数】
bias=True 模块的可学习偏置
device=None
dtype=None

9. Dropout层
防止模型过拟合，将输入的第i个样本的第j个通道以概率p独立被置零

模型优化
模型优化是指寻找最佳模型参数的过程，目标是最小化损失函数（或最大化目标函数）
主要方法【让模型在训练数据上表现更好，找到损失函数的最小值】：
    梯度下降法：沿着损失函数梯度的反方向更新参数
    随机梯度下降法（SGD）：每次用少量样本计算梯度
    Adam、RMSprop【常用】：自适应学习率的优化算法
    牛顿法、拟牛顿法：利用二阶导数信息

正则化：防止过拟合的技术，通过在损失函数中添加惩罚项来约束模型复杂度
常见方法：
    L1正则化（Lasso）:添加参数绝对值之和，可产生稀疏解
    L2正则化（Ridge）：添加参数平方和，使参数值趋于较小
    Dropout：随机丢弃神经元
    Early Stopping：在验证集性能下降前停止训练
    数据增强：扩充训练数据【如人脸识别时转脸】

损失函数 = 原始损失 + 正则化项，然后用优化算法求解



batch大小与梯度方差的关系
batch大，随机梯度的方差越小，引入的噪声越小，训练越稳定，可以设置较大的学习率
batch小，需要设置较小的学习率


模型改进：学习率 梯度【方向和步距】

参数初始化方式：让模型更快收敛，避免梯度消失/爆炸问题
1. 固定初始化参数【bias初始化0，所有神经元学到相同特征，有对称性问题】
2. 随机初始化参数
3. 预训练初始化

ReLu->He初始化
Sigmoid/Tanh->Xavier初始化 

对比学习与gan网络和自编码器对比

**迁移学习**
迁移学习：将在一个任务上学到的知识应用到另一个相关任务上的机器学习方法
特征提取：
    冻结预训练模型的大部分层，只训练最后几层分类器，适合数据很少的情况
微调：
    用**较小的学习率**更新整个模型，有一定数据量的情况
领域自适应：
    处理源域和目标域分布不同的情况

应用流程
预训练阶段：在大规模数据集上训练模型（如ImageNet）
         ↓
微调阶段：在目标任务的小数据集上调整模型
         ↓
应用：用于实际任务

## 实际运用
### 模型分析指标：
#### 分类任务

#### 回归任务

### YOLO模型结构
backbone 主干网络 
neck 颈部网络
head 头部网络
![img.png](noteSource/model structure.png "model structure")
backbone 提取特征，如ResNet、DenseNet、VGG、FCN、MobileNet
neck 对提取的特征进行处理 如ASFF、PAN、RFB、BIFPN、SPP、FPN 
head 生成预测 MaskRCNN、R-FCN、Faster RCNN


    


